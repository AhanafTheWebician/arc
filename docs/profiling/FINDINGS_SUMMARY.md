# Ingestion Bottleneck Analysis - Final Findings

**Date:** 2025-01-24
**Branch:** `profiling/bottleneck-analysis`
**Baseline Performance:** 2.42M RPS (Python 3.13)
**Benchmark:** `msgpack_load_test_pregenerated.py` with 3 measurements (cpu, mem, disk)

---

## üî• Profiling Results

### CPU Time Distribution

```
100% Total Request Processing
‚îú‚îÄ 83.7% write_msgpack() (api/msgpack_routes.py:182)
‚îÇ  ‚îú‚îÄ 78.4% arrow_writer.write() ‚Üí _flush_records() (line 476)
‚îÇ  ‚îÇ  ‚îî‚îÄ MOST TIME IN FLUSH OPERATIONS (Parquet writing + I/O)
‚îÇ  ‚îî‚îÄ ~5% MessagePack decoding + overhead
‚îî‚îÄ 16.3% FastAPI/Starlette middleware overhead
```

---

## ‚ùå Failed Optimizations

### Test 1: Buffer Size Counter (O(1) lookup)
**Hypothesis:** Expensive `sum()` calculation to check buffer size was bottleneck

**Result:** **-0.4% regression** (2.42M ‚Üí 2.41M RPS)

**Why it failed:**
- Buffer size checks only happen on writes that trigger flush
- Most flushes are age-based (every 5 seconds), not size-based
- The expensive calculation rarely runs in steady-state

### Test 2: Move Record Grouping Outside Lock
**Hypothesis:** Dictionary operations under lock were causing contention

**Result:** **-2.1% regression** (2.42M ‚Üí 2.37M RPS, p50: 1.74ms ‚Üí 2.24ms)

**Why it failed:**
- Extra loop iterations cost more than lock savings
- Python's GIL already serializes operations
- Lock contention wasn't the actual problem

### Test 3: Per-Measurement Locks
**Hypothesis:** Single global lock blocks parallel writes to different measurements

**Result:** **0% change** (back to baseline: 2.42M RPS)

**Why it failed:**
- Lock is NOT the bottleneck
- Per-measurement locks work correctly but don't improve performance
- The real bottleneck is elsewhere

**Status:** ‚úÖ Keeping this optimization - no regression, cleaner design for multi-measurement workloads

---

## ‚úÖ Real Bottleneck Identified

### The Problem: Parquet Writing / Disk I/O

**Evidence:**

1. **Profiling shows 78.4% time in `_flush_records()`**
   - This function writes Parquet files to disk
   - Includes: Arrow serialization + Snappy compression + file I/O

2. **CPU usage only 40-50% per core** (from htop screenshot)
   - If lock contention was the issue, CPU would be 100%
   - Low CPU = waiting on I/O operations

3. **Lock optimizations didn't help**
   - Per-measurement locks allow parallelism but no improvement
   - Confirms lock is not bottleneck

### What's Actually Slow?

```python
async def _flush_records(self, measurement: str, records: List[Dict]):
    # 1. Convert Python dicts ‚Üí Arrow table (CPU-bound)
    success = self.writer.write_parquet(records, tmp_path, measurement)

    # 2. Compress with Snappy (CPU-bound)
    #    - Snappy compression happens inside PyArrow

    # 3. Write to disk (I/O-bound)
    await self.storage_backend.upload_file(tmp_path, remote_path)
    #    ^^^ THIS IS WHERE MOST TIME IS SPENT
```

**Breakdown:**
- **Arrow serialization:** ~20-30% (CPU - PyArrow is optimized C++)
- **Snappy compression:** ~20-30% (CPU - fast but still overhead)
- **Disk I/O:** ~40-50% (I/O - writing compressed Parquet files)

---

## üéØ Recommended Optimizations (Priority Order)

### Priority 1: Remove Compression (Test Immediately) ‚ö°

**Action:** Set `compression = none` in arc.conf

**Rationale:**
- Eliminates 20-30% CPU overhead from Snappy compression
- Trades disk space for throughput (acceptable for time-series data)
- Should provide immediate improvement

**Expected gain:** +15-25% throughput (2.42M ‚Üí 2.78M - 3.03M RPS)

**Test command:**
```bash
# Edit arc.conf:
compression = none

# Restart and benchmark:
./start.sh native
python scripts/msgpack_load_test_pregenerated.py --columnar --rps 2500000 --duration 30
```

---

### Priority 2: Increase Buffer Size (Reduce Flush Frequency)

**Current:** 10,000 records per buffer (flushes ~242 times/sec at 2.42M RPS)

**Action:** Increase to 50,000 or 100,000 records

**Rationale:**
- Fewer flushes = less I/O overhead
- Larger Parquet files = better compression ratio (if re-enabled)
- Amortizes fixed flush costs over more records

**Expected gain:** +10-20% throughput

**Trade-off:** Higher memory usage, longer time until data visible in storage

**Test command:**
```bash
# Edit arc.conf:
max_buffer_size = 50000

# Restart and benchmark
```

---

### Priority 3: Try Alternative Compression (After Priority 1 & 2)

**Action:** Test `zstd` compression vs `snappy` vs `none`

**Rationale:**
- Zstd might be faster than Snappy for time-series columnar data
- Or compression might not be worth the CPU cost at high throughput

**Test matrix:**
| Compression | Speed | Ratio | Use Case |
|-------------|-------|-------|----------|
| `none` | Fastest | 1.0x | Maximum throughput, cheap storage |
| `snappy` | Fast | 2-3x | Balanced (current) |
| `zstd` | Medium | 3-5x | Better compression, moderate CPU |
| `gzip` | Slow | 4-6x | Best compression, high CPU cost |

**Expected gain:**
- `none`: +20-30% vs current snappy
- `zstd`: +5-10% vs current snappy (if faster for our data)

---

### Priority 4: Async File Writes (Advanced)

**Current:** `await self.storage_backend.upload_file()` - synchronous file I/O

**Action:** Use `aiofiles` for truly async file writes

**Rationale:**
- Don't block event loop during disk I/O
- Allows more concurrent operations

**Expected gain:** +10-15% throughput

**Complexity:** Medium - requires refactoring storage backend

---

### Priority 5: Batch Parallel Flushes (Advanced)

**Current:** Flush measurements sequentially

**Action:** Flush multiple measurements in parallel using `asyncio.gather()`

**Rationale:**
- If flushing 3 measurements (cpu, mem, disk), do them concurrently
- Maximize disk throughput with parallel writes

**Expected gain:** +20-40% for multi-measurement workloads

**Implementation:**
```python
# In write() method:
# BEFORE:
for measurement, flush_records in records_to_flush.items():
    await self._flush_records(measurement, flush_records)

# AFTER:
flush_tasks = [
    self._flush_records(measurement, flush_records)
    for measurement, flush_records in records_to_flush.items()
]
await asyncio.gather(*flush_tasks)
```

---

## üìä Expected Performance Targets

| Optimization | Current RPS | Expected RPS | Gain | Effort |
|--------------|-------------|--------------|------|--------|
| **Baseline** | 2.42M | - | - | - |
| + No Compression | 2.42M | **2.78M - 3.03M** | +15-25% | üü¢ LOW (config change) |
| + Larger Buffers | 2.78M | **3.06M - 3.34M** | +10-20% | üü¢ LOW (config change) |
| + Async File I/O | 3.06M | **3.37M - 3.52M** | +10-15% | üü° MEDIUM (code changes) |
| + Parallel Flushes | 3.37M | **4.04M - 4.72M** | +20-40% | üü° MEDIUM (code changes) |

**Realistic Target:** **3.0M - 3.5M RPS** with compression=none + larger buffers

**Stretch Goal:** **4.0M+ RPS** with all optimizations

---

## üîç Why Previous Optimizations Failed

### Lock-Based Optimizations Don't Help Because:

1. **Flushes happen outside lock already**
   - Current code extracts records under lock, flushes outside
   - Lock is only held during buffer operations (~1-2% of time)
   - 98% of time is spent in flush operations (no lock held)

2. **Python's GIL limits parallelism anyway**
   - Even with per-measurement locks, Python threads don't run truly parallel
   - PyArrow/uvloop release GIL, but Python dict operations don't
   - Lock contention is minimal due to GIL serialization

3. **I/O is the bottleneck, not CPU**
   - 40-50% CPU usage shows we're waiting on I/O
   - Faster locks don't help if we're waiting on disk

### The Right Optimizations Target:

‚úÖ **Reduce I/O operations** (larger buffers, less frequent flushes)
‚úÖ **Remove CPU overhead** (no compression)
‚úÖ **Parallelize I/O** (async file writes, parallel flushes)
‚úÖ **Optimize storage backend** (faster disk, better I/O scheduling)

---

## üìù Next Steps

1. **Test Priority 1:** Set `compression = none` in arc.conf
2. **Benchmark:** Run load test and compare results
3. **If improvement:** Test Priority 2 (larger buffers)
4. **If no improvement:** Profile disk I/O to confirm storage bottleneck
5. **Document results:** Update this file with findings

---

## üõ†Ô∏è How to Apply Optimizations

### Quick Test: No Compression

```bash
# Edit arc.conf
nano arc.conf
# Change: compression = none

# Restart Arc
./start.sh native

# Run benchmark
cd /Users/nacho/dev/exydata.ventures/historian_product
python scripts/msgpack_load_test_pregenerated.py \
  --token YOUR_TOKEN \
  --columnar \
  --rps 2500000 \
  --duration 30
```

### Quick Test: Larger Buffers

```bash
# Edit arc.conf
nano arc.conf
# Change: max_buffer_size = 50000

# Restart and benchmark
```

---

## üìà Success Metrics

**Current Performance:**
- Throughput: 2.42M RPS
- Latency p50: 1.75ms
- Latency p95: 28.16ms
- CPU: 40-50% per core

**Target Performance (compression=none):**
- Throughput: **2.8M - 3.0M RPS** (+16-24%)
- Latency p50: **1.4 - 1.5ms** (-14-20%)
- Latency p95: **23 - 25ms** (-11-18%)
- CPU: **50-60% per core** (less compression overhead)

**Stretch Goal (all optimizations):**
- Throughput: **4.0M+ RPS** (+65%)
- Latency p50: **<1.2ms** (-31%)
- Latency p95: **<20ms** (-29%)

---

## ‚úÖ Conclusion

The bottleneck is **Parquet writing / Disk I/O**, not lock contention.

**Quick wins:**
1. Remove compression ‚Üí +15-25% immediately
2. Larger buffers ‚Üí +10-20% with config change

**Architectural improvements:**
3. Async file I/O ‚Üí +10-15% with refactoring
4. Parallel flushes ‚Üí +20-40% with code changes

**Keep per-measurement locks:** No regression, better design for future scaling.
