# Arc Configuration File
# ======================
# Production-ready defaults for Arc time-series data platform
#
# This file uses TOML configuration format.
# Environment variables override these settings.
# Example: ARC_WORKERS=8 will override [server].workers setting

# ======================
# Server Configuration
# ======================
[server]
# Host to bind API server (default: 0.0.0.0 for all interfaces)
host = "0.0.0.0"

# Port for API server (default: 8000)
port = 8000

# Number of worker processes for high-throughput ingestion
# Recommended values:
#   1  = Single worker (~10K rps, 300-400 MB RAM)
#   4  = Light load (~40K rps, ~1.6 GB RAM)
#   8  = Medium load (~80K rps, ~3.2 GB RAM)
#   16 = High load (~160K rps, ~6.4 GB RAM)
#   0  = Auto-detect (uses CPU count)
# Default: 4 (Gunicorn multi-worker mode)
workers = 8

# Worker timeout in seconds (default: 120)
worker_timeout = 120

# Graceful shutdown timeout (default: 60)
graceful_timeout = 60

# Max requests per worker before restart (prevents memory leaks)
max_requests = 50000
max_requests_jitter = 5000

# Worker connections (concurrent requests per worker)
worker_connections = 1000

# ======================
# Authentication
# ======================
[auth]
# Enable API token authentication (recommended: true)
enabled = true

# Default API token (leave empty to auto-generate on first run)
# IMPORTANT: Change this in production!
default_token = ""

# Endpoints that don't require authentication (comma-separated)
allowlist = "/health,/ready,/docs,/openapi.json,/auth/verify"

# ======================
# Query Cache
# ======================
[query_cache]
# Enable query result caching (recommended: true for dashboards)
enabled = true

# Cache time-to-live in seconds (default: 60)
# Recommended:
#   30-60s   = Dashboards with frequent updates
#   120-300s = Dashboards with less frequent updates
ttl_seconds = 60

# Maximum number of cached queries (default: 100)
# Memory usage: ~1-5 MB per cached query
max_size = 100

# Maximum result size to cache in MB (default: 10)
# Larger results won't be cached to save memory
max_result_mb = 10

# ======================
# DuckDB Query Engine
# ======================
[duckdb]
# Connection pool size per worker (default: 5)
# Increase for high concurrent query workloads
pool_size = 5

# Maximum queries to queue when pool is full (default: 100)
# Recommended: 20-30x pool size
max_queue_size = 100

# ======================
# Data Ingestion (Line Protocol)
# ======================
[ingestion]
# Write buffer size (records before flush)
buffer_size = 10000

# Max buffer age in seconds before flush
buffer_age_seconds = 60

# Compression for parquet files (snappy, gzip, zstd, none)
compression = "snappy"

# ======================
# Storage Backend
# ======================
[storage]
# Backend type: minio, s3, gcs, ceph
backend = "minio"

# MinIO Configuration (if backend = minio)
[storage.minio]
endpoint = "http://minio:9000"
access_key = "minioadmin"
secret_key = "minioadmin123"
bucket = "arc"
prefix = ""
use_ssl = false

# AWS S3 Configuration (if backend = s3)
[storage.s3]
# access_key = ""
# secret_key = ""
# bucket = "arc-data"
# region = "us-east-1"
# prefix = ""

# GCS Configuration (if backend = gcs)
[storage.gcs]
# bucket = "arc-data"
# project_id = ""
# credentials_file = "/path/to/credentials.json"
# prefix = ""

# ======================
# Logging
# ======================
[logging]
# Log level: DEBUG, INFO, WARNING, ERROR
level = "INFO"

# Log format: structured (JSON) or text
format = "structured"

# Include stack traces in logs
include_trace = false

# ======================
# CORS (Cross-Origin Resource Sharing)
# ======================
[cors]
# Comma-separated allowed origins
# Use "*" for allow all (not recommended in production)
origins = "http://localhost:3000,http://atila:3000"

# ======================
# Monitoring & Metrics
# ======================
[monitoring]
# Enable Prometheus metrics endpoint at /metrics
# enabled = true

# Metrics collection interval in seconds
# collection_interval = 30

# ======================
# Data Source (Optional - can be configured via UI)
# ======================
[datasource]
# Type: influx, timescale, http_json
# type = "influx"

# InfluxDB 1.x Configuration
# [datasource.influx]
# host = "influxdb"
# port = 8086
# database = "historian_test"
# username = "historian"
# password = "historian123"

# ======================
# Advanced Settings
# ======================
[advanced]
# Enable experimental features
# experimental = false

# Arrow Flight server (high-performance streaming)
# arrow_flight_enabled = false
# arrow_flight_port = 8081
